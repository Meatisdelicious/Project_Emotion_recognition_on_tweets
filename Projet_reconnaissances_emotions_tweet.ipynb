{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!python -m pip install Unidecode"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93Yz3JeDoqMq",
    "outputId": "9357f53b-a98e-44b8-9867-787bbc220c3a"
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Unidecode in c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages (1.3.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ffi (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\meat is delicious\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#Import usefull package\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from unidecode import unidecode\n",
    "from csv import reader\n",
    "#Import nltk packages to manipulate text\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from nltk import word_tokenize, WordNetLemmatizer, PorterStemmer\n",
    "from nltk import NaiveBayesClassifier, classify\n",
    "from nltk import pos_tag\n",
    "from nltk import ngrams\n",
    "#***\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "# Let's add a path containing some useful nltk data\n",
    "nltk.data.path += ['/mnt/share/nltk_data']\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from functools import reduce\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# scikit-learn packages\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.metrics\n",
    "\n",
    "# word2vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kWwAqWu5bKxM",
    "outputId": "58ca3567-4ba5-4a1c-9fe3-a9beddd9b70d"
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Meat is delicious\\Anaconda3\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Meat is\n",
      "[nltk_data]     delicious\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Meat is\n",
      "[nltk_data]     delicious\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Meat is\n",
      "[nltk_data]     delicious\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Meat is\n",
      "[nltk_data]     delicious\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Meat is\n",
      "[nltk_data]     delicious\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "path=\"\"\n",
    "import re\n",
    "with open(path+'train_set.csv', 'r',encoding='utf-8') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    order = r'[0-9]'\n",
    "    Positive=[]\n",
    "    Negative=[]\n",
    "    for row in csv_reader :\n",
    "      row[1]=re.sub(order,' ',row[1])\n",
    "      row[1]=row[1].lower()\n",
    "      row[1]=unidecode(row[1])\n",
    "      if row[0]=='0':\n",
    "        Negative.append(row[1])\n",
    "      else:\n",
    "        Positive.append(row[1])"
   ],
   "metadata": {
    "id": "LknJqDjsgMKf"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_positive, test_positive = train_test_split(Positive, train_size=0.7, test_size=0.3, shuffle=True)\n",
    "train_negative, test_negative = train_test_split(Negative, train_size=0.7, test_size=0.3, shuffle=True)\n",
    "\n",
    "train_set = [(tweet,'negative','train') for tweet in train_negative] + [(tweet,'positive','train') for tweet in train_positive]\n",
    "test_set = [(tweet,'negative','test') for tweet in test_negative] + [(tweet,'positive','test') for tweet in test_positive]\n",
    "\n",
    "all_tweets=train_set+test_set\n",
    "random.shuffle(all_tweets)\n",
    "\n",
    "stoplist = stopwords.words('french')\n",
    "stop_punctuation = [':', '(', ')', '/', '|', ',',\n",
    "                    '.', '*', '#', '\"', '&', '~',\n",
    "                    '-', '_', '\\\\', '@','?','!','\\'']\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    \"\"\"\n",
    "    Modifies pos_tag to get a more general nature of word\n",
    "    \"\"\"\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return 'n'\n",
    "def preprocess(sentence):\n",
    "    \"\"\"\n",
    "    Tokenizes, lowers, and stems\n",
    "    \"\"\"\n",
    "    stemmer = SnowballStemmer('french')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized=word_tokenize(sentence)\n",
    "    POS_tagging=pos_tag(tokenized)\n",
    "    lemmatized_tweet = [lemmatizer.lemmatize(word[0],get_wordnet_pos(word[1])) for word in POS_tagging]\n",
    "\n",
    "    return [stemmer.stem(word.lower()) for word in lemmatized_tweet]\n",
    "\n",
    "def get_features(text):\n",
    "    return {word: True for word in preprocess(text) if word not in stoplist and word not in stop_punctuation}\n",
    "\n",
    "all_features = [(get_features(tweet), label, data_set) for (tweet, label,data_set) in all_tweets]"
   ],
   "metadata": {
    "id": "ulYdO1QdbWmr"
   },
   "execution_count": 5,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\Anaconda3\\lib\\genericpath.py\u001B[0m in \u001B[0;36misfile\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 30\u001B[1;33m         \u001B[0mst\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     31\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\Meat is delicious/nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-5-f4736998079b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[1;33m{\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;32mTrue\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpreprocess\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstoplist\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstop_punctuation\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m \u001B[0mall_features\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtweet\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata_set\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtweet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdata_set\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mall_tweets\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-5-f4736998079b>\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[1;33m{\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;32mTrue\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpreprocess\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstoplist\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstop_punctuation\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m \u001B[0mall_features\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtweet\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata_set\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtweet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdata_set\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mall_tweets\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m<ipython-input-5-f4736998079b>\u001B[0m in \u001B[0;36mget_features\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     40\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mget_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[1;33m{\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;32mTrue\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpreprocess\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstoplist\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mstop_punctuation\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[0mall_features\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mget_features\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtweet\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdata_set\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mtweet\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mdata_set\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mall_tweets\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-f4736998079b>\u001B[0m in \u001B[0;36mpreprocess\u001B[1;34m(sentence)\u001B[0m\n\u001B[0;32m     34\u001B[0m     \u001B[0mlemmatizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mWordNetLemmatizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     35\u001B[0m     \u001B[0mtokenized\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mword_tokenize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msentence\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 36\u001B[1;33m     \u001B[0mPOS_tagging\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpos_tag\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokenized\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     37\u001B[0m     \u001B[0mlemmatized_tweet\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mlemmatizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlemmatize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mget_wordnet_pos\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mword\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mPOS_tagging\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001B[0m in \u001B[0;36mpos_tag\u001B[1;34m(tokens, tagset, lang)\u001B[0m\n\u001B[0;32m    159\u001B[0m     \u001B[1;33m:\u001B[0m\u001B[0mrtype\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtuple\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    160\u001B[0m     \"\"\"\n\u001B[1;32m--> 161\u001B[1;33m     \u001B[0mtagger\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_get_tagger\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlang\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    162\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0m_pos_tag\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtokens\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtagset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtagger\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlang\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001B[0m in \u001B[0;36m_get_tagger\u001B[1;34m(lang)\u001B[0m\n\u001B[0;32m    105\u001B[0m         \u001B[0mtagger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0map_russian_model_loc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    106\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 107\u001B[1;33m         \u001B[0mtagger\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mPerceptronTagger\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    108\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mtagger\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, load)\u001B[0m\n\u001B[0;32m    142\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mload\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m             AP_MODEL_LOC = 'file:' + str(\n\u001B[1;32m--> 144\u001B[1;33m                 \u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'taggers/averaged_perceptron_tagger/'\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mPICKLE\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    145\u001B[0m             )\n\u001B[0;32m    146\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mAP_MODEL_LOC\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    638\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mpath_\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpaths\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    639\u001B[0m         \u001B[1;31m# Is the path item a zipfile?\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 640\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[0mpath_\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0misfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath_\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mpath_\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mendswith\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'.zip'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    641\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    642\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0mZipFilePathPointer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresource_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\genericpath.py\u001B[0m in \u001B[0;36misfile\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 30\u001B[1;33m         \u001B[0mst\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mos\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     31\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     32\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def train(features):\n",
    "    \"\"\"\n",
    "    Trains NBC on training set \n",
    "    \n",
    "    Args :\n",
    "        features (dict) : dictionnary giving words present in the verbatim\n",
    "        \n",
    "    Returns :\n",
    "        train_set : list of verbatims in the train set\n",
    "        test_set : list of verbatims in the test set\n",
    "        classifier : trained NBC model\n",
    "    \"\"\"\n",
    "    # initialize training set and test set\n",
    "    train_set = [(feature[0],feature[1]) for feature in features if feature[2]=='train']\n",
    "    test_set = [(feature[0],feature[1]) for feature in features if feature[2]=='test']\n",
    "    print ('Training set size = ' + str(len(train_set)) + ' tweets')\n",
    "    print ('Test set size = ' + str(len(test_set)) + ' tweets')\n",
    "    \n",
    "    # train the classifier\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    return train_set, test_set, classifier\n",
    "def evaluate(train_set, test_set, classifier):\n",
    "    \"\"\"\n",
    "    Checks how the classifier performs on the training and test sets\n",
    "    \"\"\"\n",
    "    print ('Accuracy on the training set = ' + str(classify.accuracy(classifier, train_set)))\n",
    "    print ('Accuracy of the test set = ' + str(classify.accuracy(classifier, test_set)))\n",
    "    return\n",
    "# Nothing to understand\n",
    "def most_informative_features_by_label(classifier, label, n=50):\n",
    "    \"\"\"\n",
    "    Returns features and their importance\n",
    "    \"\"\"\n",
    "    cpdist = classifier._feature_probdist\n",
    "    feature_list = []\n",
    "    for (fname, fval) in classifier.most_informative_features(n):\n",
    "        def labelprob(l):\n",
    "            return cpdist[l, fname].prob(fval)\n",
    "        labels = sorted([l for l in classifier._labels if fval in cpdist[l, fname].samples()], \n",
    "                        key=labelprob)\n",
    "        if labels[-1] == label:\n",
    "            feature_list.append(fname)\n",
    "    return feature_list"
   ],
   "metadata": {
    "id": "qghbOkxz09vv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_set, test_set, classifier = train(all_features)\n",
    "classifier.show_most_informative_features(10)\n",
    "evaluate(train_set, test_set, classifier)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rR0zs9DOxmJG",
    "outputId": "3354e70f-20a2-41ba-fb45-730607852eea"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "with open(path+'score_set.csv', 'r',encoding='utf-8') as read_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    Inconnu={}\n",
    "    for row in csv_reader :\n",
    "      row[1]=row[1].lower()\n",
    "      row[1]=unidecode(row[1])\n",
    "      Inconnu[row[0]]=row[1]\n"
   ],
   "metadata": {
    "id": "UhWUtEpH3QUC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "result={'sentiment':[]}\n",
    "for key in Inconnu:\n",
    "  if key!='id':\n",
    "    prediction=classifier.classify(get_features(Inconnu[key]))\n",
    "    if prediction=='positive':\n",
    "      sentiment=1\n",
    "    else:\n",
    "      sentiment=0\n",
    "    result['sentiment']+=[sentiment]\n",
    "\n",
    "df=pd.DataFrame.from_dict(result)\n",
    "df.index.name='id'\n",
    "df.to_csv(path+'out.csv',sep=';')"
   ],
   "metadata": {
    "id": "y2CNMQfk8G-p"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
